<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="WiLLM: Rethinking Wireless Systems for LLM Workloads. The first cross-layer wireless system built specifically for LLM workloads.">
  <meta property="og:title" content="WiLLM: Rethinking Wireless Systems for LLM Workloads"/>
  <meta property="og:description" content="We present WiLLM, an LLM-aware slicing architecture and cross-layer scheduling system for mobile LLM services."/>
  <meta property="og:url" content="https://openwillm.github.io"/>
  <meta property="og:image" content="static/images/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="WiLLM: Rethinking Wireless Systems for LLM Workloads">
  <meta name="twitter:description" content="WiLLM: The first cross-layer wireless system built specifically for LLM workloads.">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Large Language Model, Wireless Communication System, 5G, Network Slicing, Edge Computing, WiLLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>WiLLM: Rethinking Wireless Systems for LLM Workloads</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" style="font-size: 32px;">
              Rethinking Wireless Systems for LLM Workloads: <br>Insights, Architecture, and Implementation
            </h1>
            
            <div class="is-size-5 publication-authors">
              <span class="author-block">Anonymous</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                  
                  <span class="link-block">
                <a href="https://mega.nz/folder/dN5QDLZZ#ck8V_Wugqgsd4BstTNXRYg" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-database"></i>
                </span>
                <span>Dataset</span>
              </a>
            </span>

            <span class="link-block">
              <a href="https://github.com/OpenWiLLM/WiLLM" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>
          </span>

          <span class="link-block">
            <a href="https://github.com/OpenWiLLM/WiLLM/blob/main/Hardware.md" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="fas fa-hdd"></i>
            </span>
            <span>Hardwares</span>
          </a>
        </span>
      </div>
    </div>
  </div>
</div>
</div>
</div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large Language Models (LLMs) are becoming a core workload for emerging mobile applications such as smart glasses, real-time multimodal assistants, and on-device AR guidance. However, today's wireless systems are fundamentally mismatched to the behavior of LLM workloads.
          </p>
          <p>
            Through 1.6 million cross-layer measurements on real cellular networks, we uncover three key properties that make LLM services fundamentally different from traditional mobile workloads: (1) LLM interactions constantly shift between uploading large multimodal inputs (e.g., images) and downloading large generated outputs, causing the wireless bottleneck to flip between uplink and downlink. (2) LLM computation and wireless transmission interact in tightly coupled ways. Changes in networking slice configuration can shift the bottleneck back and forth between computation and communication. (3) LLMs generate tokens in bursty, irregular waves rather than steady streams, creating traffic patterns that existing wireless schedulers are not designed to handle.
          </p>
          <p>
            To address this mismatch, we present <strong>WiLLM</strong>, the first cross-layer wireless system built specifically for LLM workloads. WiLLM introduces an LLM-aware slicing architecture, cross-layer scheduling primitives that dynamically adapt to LLM behavior across the User Equipment (UE), RAN, and core network, and a universal UE compatibility mechanism requiring no firmware changes. We implement WiLLM with LLM-centric control paths and deploy it on a full 5G testbed with GPU-accelerated core inference. The evaluation with a baseline comparison and a smart-glasses case study demonstrates that WiLLM enables stable and predictable LLM interactions on commodity mobile devices. We release our complete platform and dataset to spur future research on LLM service for mobile systems at <a href="https://openwillm.github.io">https://openwillm.github.io</a>.
          </p>
        </div>
      </div>
    </div>
    </div>
</section>

<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Operation Video</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/4Ro0AHIUf7Y?si=fxv_5sTk41mYsj0c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title"><center>Appendix (Including <strong><span style="color:red">Online Figures</span></strong> Mentioned in the Paper)</center></h2>

      <iframe src="static/pdfs/WiLLM_Appendix.pdf" width="100%" height="1100">
        
          </iframe>
        
      </div>
    </div>
  </section>
</body>
</html>